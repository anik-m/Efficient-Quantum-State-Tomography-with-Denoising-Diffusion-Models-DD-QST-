{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:07:29.188521Z","iopub.execute_input":"2025-10-28T09:07:29.188830Z","iopub.status.idle":"2025-10-28T09:07:31.481302Z","shell.execute_reply.started":"2025-10-28T09:07:29.188801Z","shell.execute_reply":"2025-10-28T09:07:31.480069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run this in its own cell\nprint(\"--- Installing/Upgrading Qiskit packages ---\")\n!pip install --upgrade qiskit qiskit-aer qiskit-experiments qiskit-ibm-runtime matplotlib\nprint(\"--- Installation Complete ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:35:02.265818Z","iopub.execute_input":"2025-11-06T09:35:02.266165Z","iopub.status.idle":"2025-11-06T09:35:31.128968Z","shell.execute_reply.started":"2025-11-06T09:35:02.266139Z","shell.execute_reply":"2025-11-06T09:35:31.127844Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 1: Generate Training Data & Classical Baseline","metadata":{}},{"cell_type":"code","source":"# --- Installation ---\n# Run this once at the top of your notebook.\n# You may need to restart the runtime after installation.\n!pip install --upgrade --quiet qiskit qiskit-aer qiskit-experiments qiskit-ibm-runtime matplotlib ipython\n\n# --- Python Code ---\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\n# Qiskit Core\nfrom qiskit import QuantumCircuit\nfrom qiskit.quantum_info import Statevector, state_fidelity, DensityMatrix\n\n# Qiskit Aer for simulation\nfrom qiskit_aer import AerSimulator\nfrom qiskit_aer.noise import NoiseModel\n\n# Qiskit Experiments for Tomography\nfrom qiskit_experiments.library import StateTomography\n\n# Fake-backend (for realistic noise model)\nfrom qiskit_ibm_runtime.fake_provider import FakeVigoV2\n\n# Qiskit visualization utilities\nfrom qiskit.visualization import plot_state_city, plot_bloch_multivector\n\ndef main():\n    \"\"\"\n    Runs a 1-qubit state tomography experiment to establish a baseline.\n    Generates noisy 'counts' data and correctly labels them 'X', 'Y', 'Z'\n    by inspecting the gates in each circuit.\n    \"\"\"\n\n    print(\"--- Step 1: Setting up Experiment ---\")\n\n    # 1. Define the target quantum state.\n    target_state = Statevector.from_label('+')\n    print(f\"Target state |+> = {np.round(target_state.data, 3)}\")\n\n    # 2. Define simulators\n    noiseless_sim = AerSimulator()\n    print(\"Loading FakeVigo backend…\")\n    fake_backend = FakeVigoV2()\n    noise_model = NoiseModel.from_backend(fake_backend)\n\n    # 3. Build a noisy simulator\n    noisy_sim = AerSimulator(\n        noise_model = noise_model,\n        coupling_map = fake_backend.configuration().coupling_map,\n        basis_gates  = noise_model.basis_gates\n    )\n    print(\"Simulators created.\")\n\n    # 3. Setup the tomography experiment\n    qst_exp = StateTomography(target_state, measurement_indices=[0])\n\n    shots_per_basis = 1024\n    print(f\"\\nRunning StateTomography for 1-qubit state on 3 bases.\")\n    print(f\"Shots per basis: {shots_per_basis}\")\n\n    # --- Step 2: Run the experiments ---\n    print(\"\\n--- Step 2: Running Simulations ---\")\n\n    print(\"Running NOISELESS simulation...\")\n    noiseless_data = qst_exp.run(\n        noiseless_sim,\n        shots=shots_per_basis,\n        seed_simulator=42\n    ).block_for_results()\n\n    print(\"Running NOISY simulation (using FakeVigo)…\")\n    noisy_data = qst_exp.run(\n        noisy_sim,\n        shots=shots_per_basis,\n        seed_simulator=42\n    ).block_for_results()\n\n    print(\"Simulations complete.\")\n\n    # --- Step 3: Analyze tomography results ---\n    print(\"\\n--- Step 3: Analyzing Tomography Results (Classical Baseline) ---\")\n\n    noiseless_rho = noiseless_data.analysis_results(\"state\").value\n    noisy_rho = noisy_data.analysis_results(\"state\").value\n\n    noiseless_fid = state_fidelity(target_state, noiseless_rho)\n    noisy_fid     = state_fidelity(target_state, noisy_rho)\n\n    print(f\"Target Density Matrix (rho_target):\\n{DensityMatrix(target_state).data}\")\n    print(f\"\\nNoisy Reconstructed (rho_noisy):\\n{np.round(noisy_rho.data, 3)}\")\n\n    print(\"\\n--- Baseline Fidelity Scores ---\")\n    print(f\"Noiseless Fidelity: {noiseless_fid:.6f}\")\n    print(f\"Noisy Fidelity:     {noisy_fid:.6f}  <-- THIS IS THE SCORE TO BEAT\")\n\n\n    # --- Step 4: Extract training data for DDM (GATE INSPECTION FIX) ---\n    print(\"\\n--- Step 4: Extracting Data for DDM Training ---\")\n\n    # The experiment data (results)\n    circuit_data = noisy_data.data()\n    # The circuits that were run\n    experiment_circuits = qst_exp.circuits()\n    counts_by_basis = {}\n\n    if len(circuit_data) != len(experiment_circuits):\n        print(\"Error: Mismatch between number of circuits and results.\")\n    else:\n        # Loop through both lists in parallel\n        for circuit, result_data in zip(experiment_circuits, circuit_data):\n            counts = result_data.get('counts')\n\n            # Get the names of the gates in the circuit\n            # We remove 'barrier', 'save_expval', and 'measure' to find the basis gates\n            gate_names = set(\n                instr.operation.name \n                for instr in circuit.data \n                if instr.operation.name not in ['barrier', 'save_expval', 'measure', 'reset']\n            )\n\n            # Now we identify the basis by the gates present\n            basis_label = '?'\n        \n            # Correct check for the measurement basis gates\n            if 'PauliMeasX' in gate_names:\n                basis_label = 'X'\n            elif 'PauliMeasY' in gate_names:\n                basis_label = 'Y'\n            elif 'PauliMeasZ' in gate_names:\n                basis_label = 'Z'\n            else:\n                print(f\"Warning: Could not identify basis from gates: {gate_names}\")\n\n            if basis_label != '?':\n                counts_by_basis[basis_label] = counts\n            else:\n                print(f\"Warning: Failed to assign basis for circuit {circuit.name}\")\n\n    print(\"\\nTraining Data (Counts from Noisy Sim):\")\n    print(f\"Extracted {len(counts_by_basis)} 'counts' objects and sorted by basis.\")\n\n    # Ensure all bases are present, even if empty, and print in X, Y, Z order\n    all_noisy_counts = []\n    for basis in ['X', 'Y', 'Z']:\n        # Ensure counts are not None before processing\n        counts = counts_by_basis.get(basis)\n        if counts is None:\n            print(f\"Warning: No counts found for basis {basis}. Defaulting to empty.\")\n            counts = {'0': 0, '1': 0}\n        \n        all_noisy_counts.append(counts)\n        print(f\"  Basis {basis}: {counts}\")\n\n\n     # --- Step 5: Visualizing Results ---\n    print(\"\\n--- Step 5: Visualizing Results ---\")\n    \n    print(\"Displaying Bloch Sphere visualizations…\")\n    \n    print(\"Plotting Target State Bloch Sphere...\")\n    display(plot_bloch_multivector(target_state, title=\"Target State (|+>)\"))\n\n    print(\"Plotting Noisy Reconstructed Bloch Sphere...\")\n    display(plot_bloch_multivector(noisy_rho, title=f\"Noisy Reconstruction (Fid: {noisy_fid:.4f})\"))\n\n    print(\"Displaying Density Matrix 'City' plots…\")\n    \n    print(\"Plotting Target Density Matrix...\")\n    display(plot_state_city(target_state,\n                    title=\"Target Density Matrix\"))\n    \n    print(\"Plotting Noisy Reconstructed Density Matrix...\")\n    display(plot_state_city(noisy_rho,\n                    title=\"Noisy Reconstructed\"))\n\n    print(\"\\nAll visualizations complete.\")\n    print(\"\\nNext step: Copy the 'Basis X/Y/Z' counts into the Step 2 DDM script.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:08:08.772565Z","iopub.execute_input":"2025-10-28T09:08:08.772917Z","iopub.status.idle":"2025-10-28T09:08:30.598034Z","shell.execute_reply.started":"2025-10-28T09:08:08.772888Z","shell.execute_reply":"2025-10-28T09:08:30.596861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#shots = 4096 case\n# --- Installation ---\n# Run this once at the top of your notebook.\n# You may need to restart the runtime after installation.\n!pip install --upgrade --quiet qiskit qiskit-aer qiskit-experiments qiskit-ibm-runtime matplotlib ipython\n\n# --- Python Code ---\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\n# Qiskit Core\nfrom qiskit import QuantumCircuit\nfrom qiskit.quantum_info import Statevector, state_fidelity, DensityMatrix\n\n# Qiskit Aer for simulation\nfrom qiskit_aer import AerSimulator\nfrom qiskit_aer.noise import NoiseModel\n\n# Qiskit Experiments for Tomography\nfrom qiskit_experiments.library import StateTomography\n\n# Fake-backend (for realistic noise model)\nfrom qiskit_ibm_runtime.fake_provider import FakeVigoV2\n\n# Qiskit visualization utilities\nfrom qiskit.visualization import plot_state_city, plot_bloch_multivector\n\ndef main():\n    \"\"\"\n    Runs a 1-qubit state tomography experiment to establish a baseline.\n    Generates noisy 'counts' data and correctly labels them 'X', 'Y', 'Z'\n    by inspecting the gates in each circuit.\n    \"\"\"\n\n    print(\"--- Step 1: Setting up Experiment ---\")\n\n    # 1. Define the target quantum state.\n    target_state = Statevector.from_label('+')\n    print(f\"Target state |+> = {np.round(target_state.data, 3)}\")\n\n    # 2. Define simulators\n    noiseless_sim = AerSimulator()\n    print(\"Loading FakeVigo backend…\")\n    fake_backend = FakeVigoV2()\n    noise_model = NoiseModel.from_backend(fake_backend)\n\n    # 3. Build a noisy simulator\n    noisy_sim = AerSimulator(\n        noise_model = noise_model,\n        coupling_map = fake_backend.configuration().coupling_map,\n        basis_gates  = noise_model.basis_gates\n    )\n    print(\"Simulators created.\")\n\n    # 3. Setup the tomography experiment\n    qst_exp = StateTomography(target_state, measurement_indices=[0])\n\n    shots_per_basis = 4096\n    print(f\"\\nRunning StateTomography for 1-qubit state on 3 bases.\")\n    print(f\"Shots per basis: {shots_per_basis}\")\n\n    # --- Step 2: Run the experiments ---\n    print(\"\\n--- Step 2: Running Simulations ---\")\n\n    print(\"Running NOISELESS simulation...\")\n    noiseless_data = qst_exp.run(\n        noiseless_sim,\n        shots=shots_per_basis,\n        seed_simulator=42\n    ).block_for_results()\n\n    print(\"Running NOISY simulation (using FakeVigo)…\")\n    noisy_data = qst_exp.run(\n        noisy_sim,\n        shots=shots_per_basis,\n        seed_simulator=42\n    ).block_for_results()\n\n    print(\"Simulations complete.\")\n\n    # --- Step 3: Analyze tomography results ---\n    print(\"\\n--- Step 3: Analyzing Tomography Results (Classical Baseline) ---\")\n\n    noiseless_rho = noiseless_data.analysis_results(\"state\").value\n    noisy_rho = noisy_data.analysis_results(\"state\").value\n\n    noiseless_fid = state_fidelity(target_state, noiseless_rho)\n    noisy_fid     = state_fidelity(target_state, noisy_rho)\n\n    print(f\"Target Density Matrix (rho_target):\\n{DensityMatrix(target_state).data}\")\n    print(f\"\\nNoisy Reconstructed (rho_noisy):\\n{np.round(noisy_rho.data, 3)}\")\n\n    print(\"\\n--- Baseline Fidelity Scores ---\")\n    print(f\"Noiseless Fidelity: {noiseless_fid:.6f}\")\n    print(f\"Noisy Fidelity:     {noisy_fid:.6f}  <-- THIS IS THE SCORE TO BEAT\")\n\n\n    # --- Step 4: Extract training data for DDM (GATE INSPECTION FIX) ---\n    print(\"\\n--- Step 4: Extracting Data for DDM Training ---\")\n\n    # The experiment data (results)\n    circuit_data = noisy_data.data()\n    # The circuits that were run\n    experiment_circuits = qst_exp.circuits()\n    counts_by_basis = {}\n\n    if len(circuit_data) != len(experiment_circuits):\n        print(\"Error: Mismatch between number of circuits and results.\")\n    else:\n        # Loop through both lists in parallel\n        for circuit, result_data in zip(experiment_circuits, circuit_data):\n            counts = result_data.get('counts')\n\n            # Get the names of the gates in the circuit\n            # We remove 'barrier', 'save_expval', and 'measure' to find the basis gates\n            gate_names = set(\n                instr.operation.name \n                for instr in circuit.data \n                if instr.operation.name not in ['barrier', 'save_expval', 'measure', 'reset']\n            )\n\n            # Now we identify the basis by the gates present\n            basis_label = '?'\n        \n            # Correct check for the measurement basis gates\n            if 'PauliMeasX' in gate_names:\n                basis_label = 'X'\n            elif 'PauliMeasY' in gate_names:\n                basis_label = 'Y'\n            elif 'PauliMeasZ' in gate_names:\n                basis_label = 'Z'\n            else:\n                print(f\"Warning: Could not identify basis from gates: {gate_names}\")\n\n            if basis_label != '?':\n                counts_by_basis[basis_label] = counts\n            else:\n                print(f\"Warning: Failed to assign basis for circuit {circuit.name}\")\n\n    print(\"\\nTraining Data (Counts from Noisy Sim):\")\n    print(f\"Extracted {len(counts_by_basis)} 'counts' objects and sorted by basis.\")\n\n    # Ensure all bases are present, even if empty, and print in X, Y, Z order\n    all_noisy_counts = []\n    for basis in ['X', 'Y', 'Z']:\n        # Ensure counts are not None before processing\n        counts = counts_by_basis.get(basis)\n        if counts is None:\n            print(f\"Warning: No counts found for basis {basis}. Defaulting to empty.\")\n            counts = {'0': 0, '1': 0}\n        \n        all_noisy_counts.append(counts)\n        print(f\"  Basis {basis}: {counts}\")\n\n\n     # --- Step 5: Visualizing Results ---\n    print(\"\\n--- Step 5: Visualizing Results ---\")\n    \n    print(\"Displaying Bloch Sphere visualizations…\")\n    \n    print(\"Plotting Target State Bloch Sphere...\")\n    display(plot_bloch_multivector(target_state, title=\"Target State (|+>)\"))\n\n    print(\"Plotting Noisy Reconstructed Bloch Sphere...\")\n    display(plot_bloch_multivector(noisy_rho, title=f\"Noisy Reconstruction (Fid: {noisy_fid:.4f})\"))\n\n    print(\"Displaying Density Matrix 'City' plots…\")\n    \n    print(\"Plotting Target Density Matrix...\")\n    display(plot_state_city(target_state,\n                    title=\"Target Density Matrix\"))\n    \n    print(\"Plotting Noisy Reconstructed Density Matrix...\")\n    display(plot_state_city(noisy_rho,\n                    title=\"Noisy Reconstructed\"))\n\n    print(\"\\nAll visualizations complete.\")\n    print(\"\\nNext step: Copy the 'Basis X/Y/Z' counts into the Step 2 DDM script.\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:08:30.599856Z","iopub.execute_input":"2025-10-28T09:08:30.600546Z","iopub.status.idle":"2025-10-28T09:08:44.410178Z","shell.execute_reply.started":"2025-10-28T09:08:30.600505Z","shell.execute_reply":"2025-10-28T09:08:44.409059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: DDM","metadata":{}},{"cell_type":"code","source":"\n# --- Python Code ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport numpy as np\nimport collections\n\n# --- Step 1: Paste Your Data Here ---\n# TODO:\n# Copy the *actual* output from your Step 1 script and paste it below.\n# The basis order MUST be X, Y, Z.\n\n'''`\nTraining Data (Counts from Noisy Sim):\nExtracted 3 'counts' objects and sorted by basis.\n  Basis X: {'0': 940, '1': 84}\n  Basis Y: {'0': 545, '1': 479}\n  Basis Z: {'1': 497, '0': 527}\n'''\n\n\ncounts_X = {'0': 940, '1': 84}  # <-- PASTE YOUR X COUNTS (from Basis ('X',))\ncounts_Y = {'0': 545, '1': 479}  # <-- PASTE YOUR Y COUNTS (from Basis ('Y',))\ncounts_Z = {'1': 497, '0': 527} # <-- PASTE YOUR Z COUNTS (from Basis ('Z',))\n\n\n\n# --- This script continues from here ---\noriginal_noisy_counts = [counts_X, counts_Y, counts_Z]\nBASIS_LABELS = ['X', 'Y', 'Z']\n\n# --- Step 2a: Process Data for ML ---\n\nclass BitstringDataset(Dataset):\n    \"\"\"\n    Takes a Qiskit Counts dictionary and \"unrolls\" it into\n    a list of individual bitstrings (shots) for training.\n    \"\"\"\n    def __init__(self, counts_dict, basis_id):\n        self.basis_id = basis_id\n        self.data = []\n\n        # Unroll the counts\n        for bit_str, num_shots in counts_dict.items():\n            # For 1Q, bit_str is '0' or '1'. We convert to int 0 or 1.\n            if num_shots > 0:\n                bit_val = int(bit_str)\n                # Add this bit 'num_shots' times\n                self.data.extend([bit_val] * num_shots)\n        \n        # Convert to a tensor\n        self.data = torch.tensor(self.data, dtype=torch.long)\n        self.basis_tensor = torch.tensor(basis_id, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Return the clean bit and its basis ID\n        return self.data[idx], self.basis_tensor\n\n# --- Step 2b: Define the DDM Neural Network ---\n\nclass SimpleMLP(nn.Module):\n    \"\"\"\n    A simple MLP to predict noise.\n    It's *conditional* on the basis_id.\n    \"\"\"\n    def __init__(self, num_timesteps=100, num_bases=3):\n        super().__init__()\n        \n        # Embeddings for timestep and basis\n        self.time_emb = nn.Embedding(num_timesteps + 1, 32) # +1 for t=0\n        self.basis_emb = nn.Embedding(num_bases, 32)\n        \n        # Input layer: 1 (bit value) + 32 (time) + 32 (basis) = 65\n        self.net = nn.Sequential(\n            nn.Linear(1 + 32 + 32, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2) # Output: Logits for '0' and '1'\n        )\n\n    def forward(self, noisy_x, t, basis_id):\n        # x is (batch_size, 1)\n        # t is (batch_size,)\n        # basis_id is (batch_size,)\n        \n        # Get embeddings\n        t_emb = self.time_emb(t)\n        b_emb = self.basis_emb(basis_id)\n        \n        # Reshape x from (batch_size,) to (batch_size, 1) and cast to float\n        noisy_x = noisy_x.float().view(-1, 1)\n\n        # Concatenate inputs\n        x_in = torch.cat([noisy_x, t_emb, b_emb], dim=1)\n        \n        # Return logits (raw scores for each class)\n        return self.net(x_in)\n\n# --- Step 2c & 3: Define the Diffusion Model ---\n\nclass BitstringDDM:\n    \"\"\"\n    Manages the DDM training (Step 2) and sampling (Step 3) process\n    for categorical (binary) data.\n    \"\"\"\n    def __init__(self, model, num_timesteps=100, device='cuda'):\n        self.model = model.to(device)\n        self.num_timesteps = num_timesteps\n        self.device = device\n\n        # Define the noise schedule (transition matrices)\n        p_stay = torch.linspace(1.0, 0.5, num_timesteps + 1)\n        \n        self.Q = torch.zeros(num_timesteps + 1, 2, 2) # T, to_state, from_state\n        \n        for t in range(1, num_timesteps + 1):\n            p = p_stay[t]\n            # Q[t, 0, 0] = P(0 | 0) = p\n            # Q[t, 1, 0] = P(1 | 0) = 1-p\n            # Q[t, 0, 1] = P(0 | 1) = 1-p\n            # Q[t, 1, 1] = P(1 | 1) = p\n            self.Q[t] = torch.tensor([[p, 1-p], [1-p, p]])\n\n        # *** BUGFIX: *** Move Q to the device (e.g., GPU)\n        self.Q = self.Q.to(device)\n\n    def forward_diffusion(self, x_0, t):\n        \"\"\"Adds t steps of noise to a clean bit x_0.\"\"\"\n        q_t = self.Q[t] # Get the (2,2) transition matrix for timestep t\n        \n        # *** BUGFIX: ***\n        # 'q_t' is shape (2, 2).\n        # 'x_0' is shape (batch_size,).\n        # We need to select the correct column from q_t for each item in the batch.\n        # The result 'probs' should be (batch_size, 2).\n        probs = q_t[:, x_0] # This selects columns, but 't' is also a batch.\n        \n        # The 't' tensor is shape (batch_size,).\n        # 'x_0' is shape (batch_size,).\n        # We need to select Q[t_i] for each item i, then select the column x_0[i].\n        \n        # 1. Get the transition matrix for each 't' in the batch\n        # Q_t_batch shape: (batch_size, 2, 2)\n        Q_t_batch = self.Q[t]\n        \n        # 2. We need to select the column indexed by x_0 for each item in the batch.\n        # This is a bit tricky. We can use gather.\n        # Or, since it's just 2x2:\n        # P(x_t | x_0=0) is Q_t_batch[:, :, 0]\n        # P(x_t | x_0=1) is Q_t_batch[:, :, 1]\n        \n        # Create a view of x_0 for indexing\n        # x_0_idx shape: (batch_size, 1, 1)\n        x_0_idx = x_0.view(-1, 1, 1)\n        \n        # 'probs' will be (batch_size, 2)\n        # We select column 0 or 1 from the (batch_size, 2, 2) tensor\n        # We expand x_0_idx to (batch_size, 2, 1) to select the column\n        probs = Q_t_batch.gather(dim=2, index=x_0_idx.expand(-1, 2, -1)).squeeze(2)\n\n        # Sample x_t from the categorical distribution\n        x_t = torch.multinomial(probs, num_samples=1).squeeze(1)\n        return x_t.to(self.device)\n\n    def train_step(self, x_0, basis_id):\n        \"\"\"Performs one training step.\"\"\"\n        batch_size = x_0.shape[0]\n        x_0 = x_0.to(self.device)\n        basis_id = basis_id.to(self.device)\n        \n        # 1. Sample a random timestep t\n        t = torch.randint(1, self.num_timesteps + 1, (batch_size,), device=self.device)\n\n        # 2. Create noisy x_t\n        x_t = self.forward_diffusion(x_0, t)\n\n        # 3. Get model's prediction of the *clean* state (x_0)\n        pred_x_0_logits = self.model(x_t, t, basis_id)\n\n        # 4. Calculate the loss (Cross-entropy between predicted clean and actual clean)\n        loss = F.cross_entropy(pred_x_0_logits, x_0)\n        return loss\n\n    def sample(self, num_samples, basis_id):\n        \"\"\"(STEP 3) Generate new, denoised samples.\"\"\"\n        print(f\"\\n--- Step 3: Sampling {num_samples} bits for Basis {BASIS_LABELS[basis_id]} ---\")\n        \n        # 1. Start with pure noise (random bits) at time T\n        x_t = torch.randint(0, 2, (num_samples,), device=self.device)\n        \n        b_id_tensor = torch.full_like(x_t, fill_value=basis_id)\n        \n        # 2. Iteratively denoise from T down to 1\n        for t in reversed(range(1, self.num_timesteps + 1)):\n            with torch.no_grad():\n                # Get model's prediction of the clean state\n                t_tensor = torch.full_like(x_t, fill_value=t)\n                pred_x_0_logits = self.model(x_t, t_tensor, b_id_tensor)\n                \n                # Convert logits to probabilities (softmax)\n                pred_x_0_probs = F.softmax(pred_x_0_logits, dim=1)\n                \n                # Ancestral sampling step:\n                # 1. Sample the predicted clean state\n                x_0_pred = torch.multinomial(pred_x_0_probs, 1).squeeze(1)\n                \n                if t > 1:\n                    # 2. Add t-1 steps of noise to it to get x_{t-1}\n                    t_prev_tensor = torch.full_like(x_t, fill_value=t-1)\n                    x_t = self.forward_diffusion(x_0_pred, t_prev_tensor)\n                else:\n                    # Last step, the final sample is our prediction\n                    x_t = x_0_pred\n\n        # 3. Return the final denoised samples\n        return x_t.cpu().numpy()\n\n# --- Main Execution ---\n\ndef run_ddm_poc():\n    # --- Setup ---\n    NUM_EPOCHS = 200\n    BATCH_SIZE = 512\n    LEARNING_RATE = 1e-3\n    NUM_TIMESTEPS = 100\n    \n    # Use GPU if available\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n\n    # --- Step 2a (Data) ---\n    print(\"\\n--- Step 2a: Processing Data ---\")\n    print(f\"Using Original Noisy Data:\")\n    print(f\"  X: {counts_X}\")\n    print(f\"  Y: {counts_Y}\")\n    print(f\"  Z: {counts_Z}\")\n\n    dataset_x = BitstringDataset(counts_X, basis_id=0)\n    dataset_y = BitstringDataset(counts_Y, basis_id=1)\n    dataset_z = BitstringDataset(counts_Z, basis_id=2)\n    \n    # Check for empty datasets\n    if not dataset_x or not dataset_y or not dataset_z:\n        print(\"\\n*** ERROR: One or more datasets are empty. ***\")\n        print(\"Please re-run Step 1 and paste the correct non-empty 'counts' dictionaries.\")\n        return\n\n    # Combine all 3 datasets into one\n    full_dataset = ConcatDataset([dataset_x, dataset_y, dataset_z])\n    dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n    # We will sample the same number of shots we trained on\n    total_shots_per_basis = {\n        0: len(dataset_x),\n        1: len(dataset_y),\n        2: len(dataset_z)\n    }\n    \n    print(f\"Total training samples (shots): {len(full_dataset)}\")\n    print(f\"Shots per basis (for sampling): {total_shots_per_basis}\")\n\n\n    # --- Step 2b (Model) ---\n    model = SimpleMLP(num_timesteps=NUM_TIMESTEPS, num_bases=3)\n    \n    # --- Step 2c (Training) ---\n    print(\"\\n--- Step 2c: Training DDM ---\")\n    \n    ddm = BitstringDDM(model, num_timesteps=NUM_TIMESTEPS, device=device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        total_loss = 0\n        for x_0, basis_id in dataloader:\n            optimizer.zero_grad()\n            loss = ddm.train_step(x_0, basis_id)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        if (epoch + 1) % 10 == 0:\n            avg_loss = total_loss / len(dataloader)\n            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.6f}\")\n\n    print(\"--- Training Complete (Step 2 Done) ---\")\n\n    # --- Step 3: Sampling ---\n    generated_counts = []\n    \n    for basis_id in range(3):\n        # We generate the same number of samples we trained on\n        num_samples = total_shots_per_basis[basis_id]\n        samples = ddm.sample(num_samples=num_samples, basis_id=basis_id)\n        \n        # Count the bits to create a new Counts dictionary\n        # Use collections.Counter to correctly handle cases where one bit is 0\n        counts_counter = collections.Counter(samples)\n        counts = {'0': counts_counter.get(0, 0), '1': counts_counter.get(1, 0)}\n        \n        generated_counts.append(counts)\n\n    # --- Results ---\n    print(\"\\n--- Comparison: Original vs. DDM Denoised ---\")\n    for i in range(3):\n        label = BASIS_LABELS[i]\n        print(f\"\\nBasis {label}:\")\n        print(f\"  Original Noisy: {original_noisy_counts[i]}\")\n        print(f\"  DDM Denoised:   {generated_counts[i]}\")\n\n    print(\"\\nNext step (Step 4): Reconstruct a density matrix from the 'DDM Denoised' counts.\")\n\n# Run the full Proof-of-Concept\nif __name__ == \"__main__\":\n    run_ddm_poc()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:37:56.291600Z","iopub.execute_input":"2025-10-28T09:37:56.294337Z","iopub.status.idle":"2025-10-28T09:38:20.575400Z","shell.execute_reply.started":"2025-10-28T09:37:56.294281Z","shell.execute_reply":"2025-10-28T09:38:20.573515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#4096 shots\n# --- Python Code ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport numpy as np\nimport collections\n\n# --- Step 1: Paste Your Data Here ---\n# TODO:\n# Copy the *actual* output from your Step 1 script and paste it below.\n# The basis order MUST be X, Y, Z.\n\n'''`\n\nTraining Data (Counts from Noisy Sim):\nExtracted 3 'counts' objects and sorted by basis.\n  Basis X: {'0': 3785, '1': 311}\n  Basis Y: {'0': 2152, '1': 1944}\n  Basis Z: {'1': 2067, '0': 2029}\n\n'''\n\n\ncounts_X = {'0': 3785, '1': 311}  # <-- PASTE YOUR X COUNTS (from Basis ('X',))\ncounts_Y = {'0': 2152, '1': 1944}  # <-- PASTE YOUR Y COUNTS (from Basis ('Y',))\ncounts_Z = {'1': 2067, '0': 2029}  # <-- PASTE YOUR Z COUNTS (from Basis ('Z',))\n\n\n\n# --- This script continues from here ---\noriginal_noisy_counts = [counts_X, counts_Y, counts_Z]\nBASIS_LABELS = ['X', 'Y', 'Z']\n\n# --- Step 2a: Process Data for ML ---\n\nclass BitstringDataset(Dataset):\n    \"\"\"\n    Takes a Qiskit Counts dictionary and \"unrolls\" it into\n    a list of individual bitstrings (shots) for training.\n    \"\"\"\n    def __init__(self, counts_dict, basis_id):\n        self.basis_id = basis_id\n        self.data = []\n\n        # Unroll the counts\n        for bit_str, num_shots in counts_dict.items():\n            # For 1Q, bit_str is '0' or '1'. We convert to int 0 or 1.\n            if num_shots > 0:\n                bit_val = int(bit_str)\n                # Add this bit 'num_shots' times\n                self.data.extend([bit_val] * num_shots)\n        \n        # Convert to a tensor\n        self.data = torch.tensor(self.data, dtype=torch.long)\n        self.basis_tensor = torch.tensor(basis_id, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Return the clean bit and its basis ID\n        return self.data[idx], self.basis_tensor\n\n# --- Step 2b: Define the DDM Neural Network ---\n\nclass SimpleMLP(nn.Module):\n    \"\"\"\n    A simple MLP to predict noise.\n    It's *conditional* on the basis_id.\n    \"\"\"\n    def __init__(self, num_timesteps=100, num_bases=3):\n        super().__init__()\n        \n        # Embeddings for timestep and basis\n        self.time_emb = nn.Embedding(num_timesteps + 1, 32) # +1 for t=0\n        self.basis_emb = nn.Embedding(num_bases, 32)\n        \n        # Input layer: 1 (bit value) + 32 (time) + 32 (basis) = 65\n        self.net = nn.Sequential(\n            nn.Linear(1 + 32 + 32, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2) # Output: Logits for '0' and '1'\n        )\n\n    def forward(self, noisy_x, t, basis_id):\n        # x is (batch_size, 1)\n        # t is (batch_size,)\n        # basis_id is (batch_size,)\n        \n        # Get embeddings\n        t_emb = self.time_emb(t)\n        b_emb = self.basis_emb(basis_id)\n        \n        # Reshape x from (batch_size,) to (batch_size, 1) and cast to float\n        noisy_x = noisy_x.float().view(-1, 1)\n\n        # Concatenate inputs\n        x_in = torch.cat([noisy_x, t_emb, b_emb], dim=1)\n        \n        # Return logits (raw scores for each class)\n        return self.net(x_in)\n\n# --- Step 2c & 3: Define the Diffusion Model ---\n\nclass BitstringDDM:\n    \"\"\"\n    Manages the DDM training (Step 2) and sampling (Step 3) process\n    for categorical (binary) data.\n    \"\"\"\n    def __init__(self, model, num_timesteps=100, device='cuda'):\n        self.model = model.to(device)\n        self.num_timesteps = num_timesteps\n        self.device = device\n\n        # Define the noise schedule (transition matrices)\n        p_stay = torch.linspace(1.0, 0.5, num_timesteps + 1)\n        \n        self.Q = torch.zeros(num_timesteps + 1, 2, 2) # T, to_state, from_state\n        \n        for t in range(1, num_timesteps + 1):\n            p = p_stay[t]\n            # Q[t, 0, 0] = P(0 | 0) = p\n            # Q[t, 1, 0] = P(1 | 0) = 1-p\n            # Q[t, 0, 1] = P(0 | 1) = 1-p\n            # Q[t, 1, 1] = P(1 | 1) = p\n            self.Q[t] = torch.tensor([[p, 1-p], [1-p, p]])\n\n        # *** BUGFIX: *** Move Q to the device (e.g., GPU)\n        self.Q = self.Q.to(device)\n\n    def forward_diffusion(self, x_0, t):\n        \"\"\"Adds t steps of noise to a clean bit x_0.\"\"\"\n        q_t = self.Q[t] # Get the (2,2) transition matrix for timestep t\n        \n        # *** BUGFIX: ***\n        # 'q_t' is shape (2, 2).\n        # 'x_0' is shape (batch_size,).\n        # We need to select the correct column from q_t for each item in the batch.\n        # The result 'probs' should be (batch_size, 2).\n        probs = q_t[:, x_0] # This selects columns, but 't' is also a batch.\n        \n        # The 't' tensor is shape (batch_size,).\n        # 'x_0' is shape (batch_size,).\n        # We need to select Q[t_i] for each item i, then select the column x_0[i].\n        \n        # 1. Get the transition matrix for each 't' in the batch\n        # Q_t_batch shape: (batch_size, 2, 2)\n        Q_t_batch = self.Q[t]\n        \n        # 2. We need to select the column indexed by x_0 for each item in the batch.\n        # This is a bit tricky. We can use gather.\n        # Or, since it's just 2x2:\n        # P(x_t | x_0=0) is Q_t_batch[:, :, 0]\n        # P(x_t | x_0=1) is Q_t_batch[:, :, 1]\n        \n        # Create a view of x_0 for indexing\n        # x_0_idx shape: (batch_size, 1, 1)\n        x_0_idx = x_0.view(-1, 1, 1)\n        \n        # 'probs' will be (batch_size, 2)\n        # We select column 0 or 1 from the (batch_size, 2, 2) tensor\n        # We expand x_0_idx to (batch_size, 2, 1) to select the column\n        probs = Q_t_batch.gather(dim=2, index=x_0_idx.expand(-1, 2, -1)).squeeze(2)\n\n        # Sample x_t from the categorical distribution\n        x_t = torch.multinomial(probs, num_samples=1).squeeze(1)\n        return x_t.to(self.device)\n\n    def train_step(self, x_0, basis_id):\n        \"\"\"Performs one training step.\"\"\"\n        batch_size = x_0.shape[0]\n        x_0 = x_0.to(self.device)\n        basis_id = basis_id.to(self.device)\n        \n        # 1. Sample a random timestep t\n        t = torch.randint(1, self.num_timesteps + 1, (batch_size,), device=self.device)\n\n        # 2. Create noisy x_t\n        x_t = self.forward_diffusion(x_0, t)\n\n        # 3. Get model's prediction of the *clean* state (x_0)\n        pred_x_0_logits = self.model(x_t, t, basis_id)\n\n        # 4. Calculate the loss (Cross-entropy between predicted clean and actual clean)\n        loss = F.cross_entropy(pred_x_0_logits, x_0)\n        return loss\n\n    def sample(self, num_samples, basis_id):\n        \"\"\"(STEP 3) Generate new, denoised samples.\"\"\"\n        print(f\"\\n--- Step 3: Sampling {num_samples} bits for Basis {BASIS_LABELS[basis_id]} ---\")\n        \n        # 1. Start with pure noise (random bits) at time T\n        x_t = torch.randint(0, 2, (num_samples,), device=self.device)\n        \n        b_id_tensor = torch.full_like(x_t, fill_value=basis_id)\n        \n        # 2. Iteratively denoise from T down to 1\n        for t in reversed(range(1, self.num_timesteps + 1)):\n            with torch.no_grad():\n                # Get model's prediction of the clean state\n                t_tensor = torch.full_like(x_t, fill_value=t)\n                pred_x_0_logits = self.model(x_t, t_tensor, b_id_tensor)\n                \n                # Convert logits to probabilities (softmax)\n                pred_x_0_probs = F.softmax(pred_x_0_logits, dim=1)\n                \n                # Ancestral sampling step:\n                # 1. Sample the predicted clean state\n                x_0_pred = torch.multinomial(pred_x_0_probs, 1).squeeze(1)\n                \n                if t > 1:\n                    # 2. Add t-1 steps of noise to it to get x_{t-1}\n                    t_prev_tensor = torch.full_like(x_t, fill_value=t-1)\n                    x_t = self.forward_diffusion(x_0_pred, t_prev_tensor)\n                else:\n                    # Last step, the final sample is our prediction\n                    x_t = x_0_pred\n\n        # 3. Return the final denoised samples\n        return x_t.cpu().numpy()\n\n# --- Main Execution ---\n\ndef run_ddm_poc():\n    # --- Setup ---\n    NUM_EPOCHS = 200\n    BATCH_SIZE = 512\n    LEARNING_RATE = 1e-3\n    NUM_TIMESTEPS = 100\n    \n    # Use GPU if available\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n\n    # --- Step 2a (Data) ---\n    print(\"\\n--- Step 2a: Processing Data ---\")\n    print(f\"Using Original Noisy Data:\")\n    print(f\"  X: {counts_X}\")\n    print(f\"  Y: {counts_Y}\")\n    print(f\"  Z: {counts_Z}\")\n\n    dataset_x = BitstringDataset(counts_X, basis_id=0)\n    dataset_y = BitstringDataset(counts_Y, basis_id=1)\n    dataset_z = BitstringDataset(counts_Z, basis_id=2)\n    \n    # Check for empty datasets\n    if not dataset_x or not dataset_y or not dataset_z:\n        print(\"\\n*** ERROR: One or more datasets are empty. ***\")\n        print(\"Please re-run Step 1 and paste the correct non-empty 'counts' dictionaries.\")\n        return\n\n    # Combine all 3 datasets into one\n    full_dataset = ConcatDataset([dataset_x, dataset_y, dataset_z])\n    dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n    # We will sample the same number of shots we trained on\n    total_shots_per_basis = {\n        0: len(dataset_x),\n        1: len(dataset_y),\n        2: len(dataset_z)\n    }\n    \n    print(f\"Total training samples (shots): {len(full_dataset)}\")\n    print(f\"Shots per basis (for sampling): {total_shots_per_basis}\")\n\n\n    # --- Step 2b (Model) ---\n    model = SimpleMLP(num_timesteps=NUM_TIMESTEPS, num_bases=3)\n    \n    # --- Step 2c (Training) ---\n    print(\"\\n--- Step 2c: Training DDM ---\")\n    \n    ddm = BitstringDDM(model, num_timesteps=NUM_TIMESTEPS, device=device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        total_loss = 0\n        for x_0, basis_id in dataloader:\n            optimizer.zero_grad()\n            loss = ddm.train_step(x_0, basis_id)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        if (epoch + 1) % 10 == 0:\n            avg_loss = total_loss / len(dataloader)\n            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.6f}\")\n\n    print(\"--- Training Complete (Step 2 Done) ---\")\n\n    # --- Step 3: Sampling ---\n    generated_counts = []\n    \n    for basis_id in range(3):\n        # We generate the same number of samples we trained on\n        num_samples = total_shots_per_basis[basis_id]\n        samples = ddm.sample(num_samples=num_samples, basis_id=basis_id)\n        \n        # Count the bits to create a new Counts dictionary\n        # Use collections.Counter to correctly handle cases where one bit is 0\n        counts_counter = collections.Counter(samples)\n        counts = {'0': counts_counter.get(0, 0), '1': counts_counter.get(1, 0)}\n        \n        generated_counts.append(counts)\n\n    # --- Results ---\n    print(\"\\n--- Comparison: Original vs. DDM Denoised ---\")\n    for i in range(3):\n        label = BASIS_LABELS[i]\n        print(f\"\\nBasis {label}:\")\n        print(f\"  Original Noisy: {original_noisy_counts[i]}\")\n        print(f\"  DDM Denoised:   {generated_counts[i]}\")\n\n    print(\"\\nNext step (Step 4): Reconstruct a density matrix from the 'DDM Denoised' counts.\")\n\n# Run the full Proof-of-Concept\nif __name__ == \"__main__\":\n    run_ddm_poc()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:38:20.577185Z","iopub.execute_input":"2025-10-28T09:38:20.577546Z","iopub.status.idle":"2025-10-28T09:40:49.595775Z","shell.execute_reply.started":"2025-10-28T09:38:20.577520Z","shell.execute_reply":"2025-10-28T09:40:49.594573Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3: Reconstruction and FID","metadata":{}},{"cell_type":"code","source":"\n\n# --- Python Code ---\nimport numpy as np\nfrom qiskit.quantum_info import Statevector, state_fidelity, DensityMatrix\n\n# --- Pauli Matrices ---\nSIGMA_X = np.array([[0, 1], [1, 0]], dtype=complex)\nSIGMA_Y = np.array([[0, -1j], [1j, 0]], dtype=complex)\nSIGMA_Z = np.array([[1, 0], [0, -1]], dtype=complex)\nIDENTITY = np.array([[1, 0], [0, 1]], dtype=complex)\n\n# === START: TODO ===\n\n'''\n\nBasis X:\n  Original Noisy: {'0': 940, '1': 84}\n  DDM Denoised:   {'0': 931, '1': 93}\n\nBasis Y:\n  Original Noisy: {'0': 545, '1': 479}\n  DDM Denoised:   {'0': 564, '1': 460}\n\nBasis Z:\n  Original Noisy: {'1': 497, '0': 527}\n  DDM Denoised:   {'0': 445, '1': 579}\n\n'''\n\n# TODO 1:\n# Copy the 'DDM Denoised' counts from your Step 2/3 script output.\nddm_counts_X = {'0': 931, '1': 93}   # <-- PASTE YOUR DDM 'X' COUNTS\nddm_counts_Y = {'0': 564, '1': 460} # <-- PASTE YOUR DDM 'Y' COUNTS\nddm_counts_Z = {'0': 445, '1': 579}  # <-- PASTE YOUR DDM 'Z' COUNTS\n\n# TODO 2:\n# Copy the 'Noisy Fidelity' (the score to beat) from your Step 1 output.\nCLASSICAL_BASELINE_FIDELITY = 0.917969  # <-- PASTE YOUR BASELINE FIDELITY\n\n# === END: TODO ===\n\n\ndef calculate_expectation_value(counts):\n    \"\"\"\n    Calculates the expectation value <Z> from a Qiskit counts dictionary.\n    \n    <Z> = P(0) - P(1)\n    \"\"\"\n    total_shots = sum(counts.values())\n    if total_shots == 0:\n        return 0.0 # Avoid division by zero\n        \n    p_0 = counts.get('0', 0) / total_shots\n    p_1 = counts.get('1', 0) / total_shots\n    \n    return p_0 - p_1\n\ndef reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z):\n    \"\"\"\n    Reconstructs the 1-qubit density matrix rho using the formula:\n    rho = (1/2) * (I + <X>sigma_X + <Y>sigma_Y + <Z>sigma_Z)\n    \"\"\"\n    rho = 0.5 * (IDENTITY + \n                   exp_val_X * SIGMA_X + \n                   exp_val_Y * SIGMA_Y + \n                   exp_val_Z * SIGMA_Z)\n    \n    # Return as a Qiskit DensityMatrix object for fidelity calculation\n    return DensityMatrix(rho)\n\ndef run_reconstruction():\n    print(\"--- Step 4: Reconstructing DDM's Density Matrix ---\")\n\n    # 1. Define our target state again for fidelity calculation\n    target_state = Statevector.from_label('+')\n\n    # 2. Calculate expectation values from the DDM's counts\n    exp_val_X = calculate_expectation_value(ddm_counts_X)\n    exp_val_Y = calculate_expectation_value(ddm_counts_Y)\n    exp_val_Z = calculate_expectation_value(ddm_counts_Z)\n\n    print(f\"DDM Expectation Values:\")\n    print(f\"  <X> = {exp_val_X:.6f}\")\n    print(f\"  <Y> = {exp_val_Y:.6f}\")\n    print(f\"  <Z> = {exp_val_Z:.6f}\")\n\n    # 3. Reconstruct the density matrix\n    rho_ddm = reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z)\n    \n    print(f\"\\nDDM Reconstructed (rho_ddm):\\n{np.round(rho_ddm.data, 3)}\")\n\n    # 4. Calculate the final fidelity\n    ddm_fidelity = state_fidelity(target_state, rho_ddm)\n\n    print(\"\\n--- Step 5: Final Results & Comparison ---\")\n    print(f\"  Classical Baseline Fidelity: {CLASSICAL_BASELINE_FIDELITY:.6f}\")\n    print(f\"  DDM Denoised Fidelity:       {ddm_fidelity:.6f}\")\n\n    # 5. Print the verdict\n    if ddm_fidelity > CLASSICAL_BASELINE_FIDELITY:\n        improvement = ddm_fidelity - CLASSICAL_BASELINE_FIDELITY\n        print(f\"\\n*** SUCCESS! ***\")\n        print(f\"The DDM improved the fidelity by {improvement:.6f}.\")\n    else:\n        print(f\"\\n--- No Improvement ---\")\n        print(\"The DDM fidelity did not beat the classical baseline.\")\n        print(\"This may be due to a short training run or unlucky sampling.\")\n        print(\"Try re-running Step 2/3, or increasing NUM_EPOCHS.\")\n\nif __name__ == \"__main__\":\n    run_reconstruction()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:40:49.596959Z","iopub.execute_input":"2025-10-28T09:40:49.597295Z","iopub.status.idle":"2025-10-28T09:40:49.616833Z","shell.execute_reply.started":"2025-10-28T09:40:49.597270Z","shell.execute_reply":"2025-10-28T09:40:49.615452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for 4096 shots\n\n# --- Python Code ---\nimport numpy as np\nfrom qiskit.quantum_info import Statevector, state_fidelity, DensityMatrix\n\n# --- Pauli Matrices ---\nSIGMA_X = np.array([[0, 1], [1, 0]], dtype=complex)\nSIGMA_Y = np.array([[0, -1j], [1j, 0]], dtype=complex)\nSIGMA_Z = np.array([[1, 0], [0, -1]], dtype=complex)\nIDENTITY = np.array([[1, 0], [0, 1]], dtype=complex)\n\n# === START: TODO ===\n\n'''\nBasis X:\n  Original Noisy: {'0': 3785, '1': 311}\n  DDM Denoised:   {'0': 3602, '1': 494}\n\nBasis Y:\n  Original Noisy: {'0': 2152, '1': 1944}\n  DDM Denoised:   {'0': 2068, '1': 2028}\n\nBasis Z:\n  Original Noisy: {'1': 2067, '0': 2029}\n  DDM Denoised:   {'0': 1741, '1': 2355}\n\n'''\n\n# TODO 1:\n# Copy the 'DDM Denoised' counts from your Step 2/3 script output.\nddm_counts_X = {'0': 3602, '1': 494}   # <-- PASTE YOUR DDM 'X' COUNTS\nddm_counts_Y = {'0': 2068, '1': 2028}  # <-- PASTE YOUR DDM 'Y' COUNTS\nddm_counts_Z = {'0': 1741, '1': 2355}  # <-- PASTE YOUR DDM 'Z' COUNTS\n\n# TODO 2:\n# Copy the 'Noisy Fidelity' (the score to beat) from your Step 1 output.\nCLASSICAL_BASELINE_FIDELITY = 0.924072  # <-- PASTE YOUR BASELINE FIDELITY\n\n# === END: TODO ===\n\n\ndef calculate_expectation_value(counts):\n    \"\"\"\n    Calculates the expectation value <Z> from a Qiskit counts dictionary.\n    \n    <Z> = P(0) - P(1)\n    \"\"\"\n    total_shots = sum(counts.values())\n    if total_shots == 0:\n        return 0.0 # Avoid division by zero\n        \n    p_0 = counts.get('0', 0) / total_shots\n    p_1 = counts.get('1', 0) / total_shots\n    \n    return p_0 - p_1\n\ndef reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z):\n    \"\"\"\n    Reconstructs the 1-qubit density matrix rho using the formula:\n    rho = (1/2) * (I + <X>sigma_X + <Y>sigma_Y + <Z>sigma_Z)\n    \"\"\"\n    rho = 0.5 * (IDENTITY + \n                   exp_val_X * SIGMA_X + \n                   exp_val_Y * SIGMA_Y + \n                   exp_val_Z * SIGMA_Z)\n    \n    # Return as a Qiskit DensityMatrix object for fidelity calculation\n    return DensityMatrix(rho)\n\ndef run_reconstruction():\n    print(\"--- Step 4: Reconstructing DDM's Density Matrix ---\")\n\n    # 1. Define our target state again for fidelity calculation\n    target_state = Statevector.from_label('+')\n\n    # 2. Calculate expectation values from the DDM's counts\n    exp_val_X = calculate_expectation_value(ddm_counts_X)\n    exp_val_Y = calculate_expectation_value(ddm_counts_Y)\n    exp_val_Z = calculate_expectation_value(ddm_counts_Z)\n\n    print(f\"DDM Expectation Values:\")\n    print(f\"  <X> = {exp_val_X:.6f}\")\n    print(f\"  <Y> = {exp_val_Y:.6f}\")\n    print(f\"  <Z> = {exp_val_Z:.6f}\")\n\n    # 3. Reconstruct the density matrix\n    rho_ddm = reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z)\n    \n    print(f\"\\nDDM Reconstructed (rho_ddm):\\n{np.round(rho_ddm.data, 3)}\")\n\n    # 4. Calculate the final fidelity\n    ddm_fidelity = state_fidelity(target_state, rho_ddm)\n\n    print(\"\\n--- Step 5: Final Results & Comparison ---\")\n    print(f\"  Classical Baseline Fidelity: {CLASSICAL_BASELINE_FIDELITY:.6f}\")\n    print(f\"  DDM Denoised Fidelity:       {ddm_fidelity:.6f}\")\n\n    # 5. Print the verdict\n    if ddm_fidelity > CLASSICAL_BASELINE_FIDELITY:\n        improvement = ddm_fidelity - CLASSICAL_BASELINE_FIDELITY\n        print(f\"\\n*** SUCCESS! ***\")\n        print(f\"The DDM improved the fidelity by {improvement:.6f}.\")\n    else:\n        print(f\"\\n--- No Improvement ---\")\n        print(\"The DDM fidelity did not beat the classical baseline.\")\n        print(\"This may be due to a short training run or unlucky sampling.\")\n        print(\"Try re-running Step 2/3, or increasing NUM_EPOCHS.\")\n\nif __name__ == \"__main__\":\n    run_reconstruction()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T09:40:49.619162Z","iopub.execute_input":"2025-10-28T09:40:49.619491Z","iopub.status.idle":"2025-10-28T09:40:49.643797Z","shell.execute_reply.started":"2025-10-28T09:40:49.619469Z","shell.execute_reply":"2025-10-28T09:40:49.642425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Second iteration of steps 2 and 3 using a better model","metadata":{}},{"cell_type":"code","source":"# Step 2 with more powerful model\n\n# --- Python Code ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport numpy as np\nimport collections\n\n# --- Step 1: Paste Your Data Here ---\n# TODO:\n# Copy the *actual* output from your Step 1 script and paste it below.\n# The basis order MUST be X, Y, Z.\n\n'''\n  Basis X: {'0': 940, '1': 84}\n  Basis Y: {'0': 545, '1': 479}\n  Basis Z: {'1': 497, '0': 527}\n'''\n\ncounts_X = {'0': 940, '1': 84}  # <-- PASTE YOUR X COUNTS\ncounts_Y = {'0': 545, '1': 479}  # <-- PASTE YOUR Y COUNTS\ncounts_Z = {'0': 497, '1': 527}  # <-- PASTE YOUR Z COUNTS\n\n\n# --- This script continues from here ---\noriginal_noisy_counts = [counts_X, counts_Y, counts_Z]\nBASIS_LABELS = ['X', 'Y', 'Z']\n\n# --- Step 2a: Process Data for ML ---\n\nclass BitstringDataset(Dataset):\n    \"\"\"\n    Takes a Qiskit Counts dictionary and \"unrolls\" it into\n    a list of individual bitstrings (shots) for training.\n    \"\"\"\n    def __init__(self, counts_dict, basis_id):\n        self.basis_id = basis_id\n        self.data = []\n\n        # Unroll the counts\n        for bit_str, num_shots in counts_dict.items():\n            if num_shots > 0:\n                bit_val = int(bit_str)\n                self.data.extend([bit_val] * num_shots)\n        \n        self.data = torch.tensor(self.data, dtype=torch.long)\n        self.basis_tensor = torch.tensor(basis_id, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.basis_tensor\n\n# --- Step 2b: Define the (UPGRADED) DDM Neural Network ---\n\nclass UpgradedMLP(nn.Module):\n    \"\"\"\n    A more powerful MLP to predict noise.\n    It's *conditional* on the basis_id.\n    \n    *** CHANGES: ***\n    - Increased embedding dimensions (32 -> 128)\n    - Deeper and wider network (128 -> 256)\n    - Added an extra layer\n    \"\"\"\n    def __init__(self, num_timesteps=100, num_bases=3):\n        super().__init__()\n        \n        EMBED_DIM = 128  # Was 32\n        \n        self.time_emb = nn.Embedding(num_timesteps + 1, EMBED_DIM) # +1 for t=0\n        self.basis_emb = nn.Embedding(num_bases, EMBED_DIM)\n        \n        # Input layer: 1 (bit) + 128 (time) + 128 (basis) = 257\n        HIDDEN_DIM = 256 # Was 128\n        \n        self.net = nn.Sequential(\n            nn.Linear(1 + EMBED_DIM + EMBED_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM), # New layer\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM), # Was 128\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, 2) # Output: Logits for '0' and '1'\n        )\n\n    def forward(self, noisy_x, t, basis_id):\n        t_emb = self.time_emb(t)\n        b_emb = self.basis_emb(basis_id)\n        noisy_x = noisy_x.float().view(-1, 1)\n        x_in = torch.cat([noisy_x, t_emb, b_emb], dim=1)\n        return self.net(x_in)\n\n# --- Step 2c & 3: Define the Diffusion Model ---\n\nclass BitstringDDM:\n    \"\"\"\n    Manages the DDM training (Step 2) and sampling (Step 3) process.\n    (This class is unchanged, but will use the new UpgradedMLP)\n    \"\"\"\n    def __init__(self, model, num_timesteps=100, device='cuda'):\n        self.model = model.to(device)\n        self.num_timesteps = num_timesteps\n        self.device = device\n\n        # Define the noise schedule (transition matrices)\n        p_stay = torch.linspace(1.0, 0.5, num_timesteps + 1)\n        \n        self.Q = torch.zeros(num_timesteps + 1, 2, 2) # T, to_state, from_state\n        \n        for t in range(1, num_timesteps + 1):\n            p = p_stay[t]\n            self.Q[t] = torch.tensor([[p, 1-p], [1-p, p]])\n\n        self.Q = self.Q.to(device)\n\n    def forward_diffusion(self, x_0, t):\n        \"\"\"Adds t steps of noise to a clean bit x_0.\"\"\"\n        Q_t_batch = self.Q[t]\n        x_0_idx = x_0.view(-1, 1, 1)\n        probs = Q_t_batch.gather(dim=2, index=x_0_idx.expand(-1, 2, -1)).squeeze(2)\n        x_t = torch.multinomial(probs, num_samples=1).squeeze(1)\n        return x_t.to(self.device)\n\n    def train_step(self, x_0, basis_id):\n        \"\"\"Performs one training step.\"\"\"\n        batch_size = x_0.shape[0]\n        x_0 = x_0.to(self.device)\n        basis_id = basis_id.to(self.device)\n        \n        t = torch.randint(1, self.num_timesteps + 1, (batch_size,), device=self.device)\n        x_t = self.forward_diffusion(x_0, t)\n        pred_x_0_logits = self.model(x_t, t, basis_id)\n        loss = F.cross_entropy(pred_x_0_logits, x_0)\n        return loss\n\n    def sample(self, num_samples, basis_id):\n        \"\"\"(STEP 3) Generate new, denoised samples.\"\"\"\n        print(f\"\\n--- Step 3: Sampling {num_samples} bits for Basis {BASIS_LABELS[basis_id]} ---\")\n        \n        x_t = torch.randint(0, 2, (num_samples,), device=self.device)\n        b_id_tensor = torch.full_like(x_t, fill_value=basis_id)\n        \n        for t in reversed(range(1, self.num_timesteps + 1)):\n            with torch.no_grad():\n                t_tensor = torch.full_like(x_t, fill_value=t)\n                pred_x_0_logits = self.model(x_t, t_tensor, b_id_tensor)\n                pred_x_0_probs = F.softmax(pred_x_0_logits, dim=1)\n                x_0_pred = torch.multinomial(pred_x_0_probs, 1).squeeze(1)\n                \n                if t > 1:\n                    t_prev_tensor = torch.full_like(x_t, fill_value=t-1)\n                    x_t = self.forward_diffusion(x_0_pred, t_prev_tensor)\n                else:\n                    x_t = x_0_pred\n\n        return x_t.cpu().numpy()\n\n# --- Main Execution ---\n\ndef run_ddm_poc():\n    # --- Setup ---\n    # *** CHANGES: ***\n    # - NUM_EPOCHS = 300 (Keep it high)\n    # - BATCH_SIZE = 128 (Smaller for more updates)\n    # - LEARNING_RATE = 3e-4 (Slightly lower for stability)\n    NUM_EPOCHS = 300\n    BATCH_SIZE = 128\n    LEARNING_RATE = 3e-4 \n    NUM_TIMESTEPS = 100\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n\n    # --- Step 2a (Data) ---\n    print(\"\\n--- Step 2a: Processing Data ---\")\n    print(f\"Using Original Noisy Data:\")\n    print(f\"  X: {counts_X}\")\n    print(f\"  Y: {counts_Y}\")\n    print(f\"  Z: {counts_Z}\")\n\n    dataset_x = BitstringDataset(counts_X, basis_id=0)\n    dataset_y = BitstringDataset(counts_Y, basis_id=1)\n    dataset_z = BitstringDataset(counts_Z, basis_id=2)\n    \n    if len(dataset_x) == 0 or len(dataset_y) == 0 or len(dataset_z) == 0:\n        print(\"\\n*** ERROR: One or more datasets are empty. ***\")\n        print(\"Please re-run Step 1 and paste the correct non-empty 'counts' dictionaries.\")\n        return\n\n    full_dataset = ConcatDataset([dataset_x, dataset_y, dataset_z])\n    dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n    total_shots_per_basis = {\n        0: len(dataset_x),\n        1: len(dataset_y),\n        2: len(dataset_z)\n    }\n    \n    print(f\"Total training samples (shots): {len(full_dataset)}\")\n    print(f\"Shots per basis (for sampling): {total_shots_per_basis}\")\n\n\n    # --- Step 2b (Model) ---\n    # *** CHANGE: Using new, more powerful model ***\n    model = UpgradedMLP(num_timesteps=NUM_TIMESTEPS, num_bases=3)\n    \n    # --- Step 2c (Training) ---\n    print(\"\\n--- Step 2c: Training DDM (Upgraded Model) ---\")\n    \n    ddm = BitstringDDM(model, num_timesteps=NUM_TIMESTEPS, device=device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        total_loss = 0\n        for x_0, basis_id in dataloader:\n            optimizer.zero_grad()\n            loss = ddm.train_step(x_0, basis_id)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        if (epoch + 1) % 25 == 0: # Print every 25 epochs\n            avg_loss = total_loss / len(dataloader)\n            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.6f}\")\n\n    print(\"--- Training Complete (Step 2 Done) ---\")\n\n    # --- Step 3: Sampling ---\n    generated_counts = []\n    \n    for basis_id in range(3):\n        num_samples = total_shots_per_basis[basis_id]\n        samples = ddm.sample(num_samples=num_samples, basis_id=basis_id)\n        \n        counts_counter = collections.Counter(samples)\n        counts = {'0': counts_counter.get(0, 0), '1': counts_counter.get(1, 0)}\n        \n        generated_counts.append(counts)\n\n    # --- Results ---\n    print(\"\\n--- Comparison: Original vs. DDM Denoised ---\")\n    for i in range(3):\n        label = BASIS_LABELS[i]\n        print(f\"\\nBasis {label}:\")\n        print(f\"  Original Noisy: {original_noisy_counts[i]}\")\n        print(f\"  DDM Denoised:   {generated_counts[i]}\")\n\n    print(\"\\nNext step (Step 4): Reconstruct a density matrix from the 'DDM Denoised' counts.\")\n\nif __name__ == \"__main__\":\n    run_ddm_poc()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:31:50.702441Z","iopub.execute_input":"2025-11-06T09:31:50.703471Z","iopub.status.idle":"2025-11-06T09:32:42.122741Z","shell.execute_reply.started":"2025-11-06T09:31:50.703440Z","shell.execute_reply":"2025-11-06T09:32:42.121784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# --- Python Code ---\nimport numpy as np\nfrom qiskit.quantum_info import Statevector, state_fidelity, DensityMatrix\n\n# --- Pauli Matrices ---\nSIGMA_X = np.array([[0, 1], [1, 0]], dtype=complex)\nSIGMA_Y = np.array([[0, -1j], [1j, 0]], dtype=complex)\nSIGMA_Z = np.array([[1, 0], [0, -1]], dtype=complex)\nIDENTITY = np.array([[1, 0], [0, 1]], dtype=complex)\n\n# === START: TODO ===\n\n'''\n\n\nBasis X:\n  Original Noisy: {'0': 940, '1': 84}\n  DDM Denoised:   {'0': 952, '1': 72}\n\nBasis Y:\n  Original Noisy: {'0': 545, '1': 479}\n  DDM Denoised:   {'0': 525, '1': 499}\n\nBasis Z:\n  Original Noisy: {'0': 497, '1': 527}\n  DDM Denoised:   {'0': 454, '1': 570}\n\n'''\n\n# TODO 1:\n# Copy the 'DDM Denoised' counts from your Step 2/3 script output.\nddm_counts_X = {'0': 952, '1': 84}   # <-- PASTE YOUR DDM 'X' COUNTS\nddm_counts_Y = {'0': 525, '1': 499} # <-- PASTE YOUR DDM 'Y' COUNTS\nddm_counts_Z = {'0': 454, '1': 570}  # <-- PASTE YOUR DDM 'Z' COUNTS\n\n# TODO 2:\n# Copy the 'Noisy Fidelity' (the score to beat) from your Step 1 output.\nCLASSICAL_BASELINE_FIDELITY = 0.917969  # <-- PASTE YOUR BASELINE FIDELITY\n\n# === END: TODO ===\n\n\ndef calculate_expectation_value(counts):\n    \"\"\"\n    Calculates the expectation value <Z> from a Qiskit counts dictionary.\n    \n    <Z> = P(0) - P(1)\n    \"\"\"\n    total_shots = sum(counts.values())\n    if total_shots == 0:\n        return 0.0 # Avoid division by zero\n        \n    p_0 = counts.get('0', 0) / total_shots\n    p_1 = counts.get('1', 0) / total_shots\n    \n    return p_0 - p_1\n\ndef reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z):\n    \"\"\"\n    Reconstructs the 1-qubit density matrix rho using the formula:\n    rho = (1/2) * (I + <X>sigma_X + <Y>sigma_Y + <Z>sigma_Z)\n    \"\"\"\n    rho = 0.5 * (IDENTITY + \n                   exp_val_X * SIGMA_X + \n                   exp_val_Y * SIGMA_Y + \n                   exp_val_Z * SIGMA_Z)\n    \n    # Return as a Qiskit DensityMatrix object for fidelity calculation\n    return DensityMatrix(rho)\n\ndef run_reconstruction():\n    print(\"--- Step 4: Reconstructing DDM's Density Matrix ---\")\n\n    # 1. Define our target state again for fidelity calculation\n    target_state = Statevector.from_label('+')\n\n    # 2. Calculate expectation values from the DDM's counts\n    exp_val_X = calculate_expectation_value(ddm_counts_X)\n    exp_val_Y = calculate_expectation_value(ddm_counts_Y)\n    exp_val_Z = calculate_expectation_value(ddm_counts_Z)\n\n    print(f\"DDM Expectation Values:\")\n    print(f\"  <X> = {exp_val_X:.6f}\")\n    print(f\"  <Y> = {exp_val_Y:.6f}\")\n    print(f\"  <Z> = {exp_val_Z:.6f}\")\n\n    # 3. Reconstruct the density matrix\n    rho_ddm = reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z)\n    \n    print(f\"\\nDDM Reconstructed (rho_ddm):\\n{np.round(rho_ddm.data, 3)}\")\n\n    # 4. Calculate the final fidelity\n    ddm_fidelity = state_fidelity(target_state, rho_ddm)\n\n    print(\"\\n--- Step 5: Final Results & Comparison ---\")\n    print(f\"  Classical Baseline Fidelity: {CLASSICAL_BASELINE_FIDELITY:.6f}\")\n    print(f\"  DDM Denoised Fidelity:       {ddm_fidelity:.6f}\")\n\n    # 5. Print the verdict\n    if ddm_fidelity > CLASSICAL_BASELINE_FIDELITY:\n        improvement = ddm_fidelity - CLASSICAL_BASELINE_FIDELITY\n        print(f\"\\n*** SUCCESS! ***\")\n        print(f\"The DDM improved the fidelity by {improvement:.6f}.\")\n    else:\n        print(f\"\\n--- No Improvement ---\")\n        print(\"The DDM fidelity did not beat the classical baseline.\")\n        print(\"This may be due to a short training run or unlucky sampling.\")\n        print(\"Try re-running Step 2/3, or increasing NUM_EPOCHS.\")\n\nif __name__ == \"__main__\":\n    run_reconstruction()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:35:42.607105Z","iopub.execute_input":"2025-11-06T09:35:42.607456Z","iopub.status.idle":"2025-11-06T09:35:43.173745Z","shell.execute_reply.started":"2025-11-06T09:35:42.607429Z","shell.execute_reply":"2025-11-06T09:35:43.172704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#4096\n# Step 2 with more powerful model\n\n# --- Python Code ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nimport numpy as np\nimport collections\n\n# --- Step 1: Paste Your Data Here ---\n# TODO:\n# Copy the *actual* output from your Step 1 script and paste it below.\n# The basis order MUST be X, Y, Z.\n\n'''\n  Basis X: {'0': 3785, '1': 311}\n  Basis Y: {'0': 2152, '1': 1944}\n  Basis Z: {'1': 2067, '0': 2029}\n'''\n\ncounts_X = {'0': 3785, '1': 311}  # <-- PASTE YOUR X COUNTS\ncounts_Y = {'0': 2152, '1': 1944}  # <-- PASTE YOUR Y COUNTS\ncounts_Z = {'0': 2067, '1': 2029}  # <-- PASTE YOUR Z COUNTS\n\n\n# --- This script continues from here ---\noriginal_noisy_counts = [counts_X, counts_Y, counts_Z]\nBASIS_LABELS = ['X', 'Y', 'Z']\n\n# --- Step 2a: Process Data for ML ---\n\nclass BitstringDataset(Dataset):\n    \"\"\"\n    Takes a Qiskit Counts dictionary and \"unrolls\" it into\n    a list of individual bitstrings (shots) for training.\n    \"\"\"\n    def __init__(self, counts_dict, basis_id):\n        self.basis_id = basis_id\n        self.data = []\n\n        # Unroll the counts\n        for bit_str, num_shots in counts_dict.items():\n            if num_shots > 0:\n                bit_val = int(bit_str)\n                self.data.extend([bit_val] * num_shots)\n        \n        self.data = torch.tensor(self.data, dtype=torch.long)\n        self.basis_tensor = torch.tensor(basis_id, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.basis_tensor\n\n# --- Step 2b: Define the (UPGRADED) DDM Neural Network ---\n\nclass UpgradedMLP(nn.Module):\n    \"\"\"\n    A more powerful MLP to predict noise.\n    It's *conditional* on the basis_id.\n    \n    *** CHANGES: ***\n    - Increased embedding dimensions (32 -> 128)\n    - Deeper and wider network (128 -> 256)\n    - Added an extra layer\n    \"\"\"\n    def __init__(self, num_timesteps=100, num_bases=3):\n        super().__init__()\n        \n        EMBED_DIM = 128  # Was 32\n        \n        self.time_emb = nn.Embedding(num_timesteps + 1, EMBED_DIM) # +1 for t=0\n        self.basis_emb = nn.Embedding(num_bases, EMBED_DIM)\n        \n        # Input layer: 1 (bit) + 128 (time) + 128 (basis) = 257\n        HIDDEN_DIM = 256 # Was 128\n        \n        self.net = nn.Sequential(\n            nn.Linear(1 + EMBED_DIM + EMBED_DIM, HIDDEN_DIM),\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM), # New layer\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, HIDDEN_DIM), # Was 128\n            nn.ReLU(),\n            nn.Linear(HIDDEN_DIM, 2) # Output: Logits for '0' and '1'\n        )\n\n    def forward(self, noisy_x, t, basis_id):\n        t_emb = self.time_emb(t)\n        b_emb = self.basis_emb(basis_id)\n        noisy_x = noisy_x.float().view(-1, 1)\n        x_in = torch.cat([noisy_x, t_emb, b_emb], dim=1)\n        return self.net(x_in)\n\n# --- Step 2c & 3: Define the Diffusion Model ---\n\nclass BitstringDDM:\n    \"\"\"\n    Manages the DDM training (Step 2) and sampling (Step 3) process.\n    (This class is unchanged, but will use the new UpgradedMLP)\n    \"\"\"\n    def __init__(self, model, num_timesteps=100, device='cuda'):\n        self.model = model.to(device)\n        self.num_timesteps = num_timesteps\n        self.device = device\n\n        # Define the noise schedule (transition matrices)\n        p_stay = torch.linspace(1.0, 0.5, num_timesteps + 1)\n        \n        self.Q = torch.zeros(num_timesteps + 1, 2, 2) # T, to_state, from_state\n        \n        for t in range(1, num_timesteps + 1):\n            p = p_stay[t]\n            self.Q[t] = torch.tensor([[p, 1-p], [1-p, p]])\n\n        self.Q = self.Q.to(device)\n\n    def forward_diffusion(self, x_0, t):\n        \"\"\"Adds t steps of noise to a clean bit x_0.\"\"\"\n        Q_t_batch = self.Q[t]\n        x_0_idx = x_0.view(-1, 1, 1)\n        probs = Q_t_batch.gather(dim=2, index=x_0_idx.expand(-1, 2, -1)).squeeze(2)\n        x_t = torch.multinomial(probs, num_samples=1).squeeze(1)\n        return x_t.to(self.device)\n\n    def train_step(self, x_0, basis_id):\n        \"\"\"Performs one training step.\"\"\"\n        batch_size = x_0.shape[0]\n        x_0 = x_0.to(self.device)\n        basis_id = basis_id.to(self.device)\n        \n        t = torch.randint(1, self.num_timesteps + 1, (batch_size,), device=self.device)\n        x_t = self.forward_diffusion(x_0, t)\n        pred_x_0_logits = self.model(x_t, t, basis_id)\n        loss = F.cross_entropy(pred_x_0_logits, x_0)\n        return loss\n\n    def sample(self, num_samples, basis_id):\n        \"\"\"(STEP 3) Generate new, denoised samples.\"\"\"\n        print(f\"\\n--- Step 3: Sampling {num_samples} bits for Basis {BASIS_LABELS[basis_id]} ---\")\n        \n        x_t = torch.randint(0, 2, (num_samples,), device=self.device)\n        b_id_tensor = torch.full_like(x_t, fill_value=basis_id)\n        \n        for t in reversed(range(1, self.num_timesteps + 1)):\n            with torch.no_grad():\n                t_tensor = torch.full_like(x_t, fill_value=t)\n                pred_x_0_logits = self.model(x_t, t_tensor, b_id_tensor)\n                pred_x_0_probs = F.softmax(pred_x_0_logits, dim=1)\n                x_0_pred = torch.multinomial(pred_x_0_probs, 1).squeeze(1)\n                \n                if t > 1:\n                    t_prev_tensor = torch.full_like(x_t, fill_value=t-1)\n                    x_t = self.forward_diffusion(x_0_pred, t_prev_tensor)\n                else:\n                    x_t = x_0_pred\n\n        return x_t.cpu().numpy()\n\n# --- Main Execution ---\n\ndef run_ddm_poc():\n    # --- Setup ---\n    # *** CHANGES: ***\n    # - NUM_EPOCHS = 300 (Keep it high)\n    # - BATCH_SIZE = 128 (Smaller for more updates)\n    # - LEARNING_RATE = 3e-4 (Slightly lower for stability)\n    NUM_EPOCHS = 300\n    BATCH_SIZE = 128\n    LEARNING_RATE = 3e-4 \n    NUM_TIMESTEPS = 100\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n\n    # --- Step 2a (Data) ---\n    print(\"\\n--- Step 2a: Processing Data ---\")\n    print(f\"Using Original Noisy Data:\")\n    print(f\"  X: {counts_X}\")\n    print(f\"  Y: {counts_Y}\")\n    print(f\"  Z: {counts_Z}\")\n\n    dataset_x = BitstringDataset(counts_X, basis_id=0)\n    dataset_y = BitstringDataset(counts_Y, basis_id=1)\n    dataset_z = BitstringDataset(counts_Z, basis_id=2)\n    \n    if len(dataset_x) == 0 or len(dataset_y) == 0 or len(dataset_z) == 0:\n        print(\"\\n*** ERROR: One or more datasets are empty. ***\")\n        print(\"Please re-run Step 1 and paste the correct non-empty 'counts' dictionaries.\")\n        return\n\n    full_dataset = ConcatDataset([dataset_x, dataset_y, dataset_z])\n    dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n    total_shots_per_basis = {\n        0: len(dataset_x),\n        1: len(dataset_y),\n        2: len(dataset_z)\n    }\n    \n    print(f\"Total training samples (shots): {len(full_dataset)}\")\n    print(f\"Shots per basis (for sampling): {total_shots_per_basis}\")\n\n\n    # --- Step 2b (Model) ---\n    # *** CHANGE: Using new, more powerful model ***\n    model = UpgradedMLP(num_timesteps=NUM_TIMESTEPS, num_bases=3)\n    \n    # --- Step 2c (Training) ---\n    print(\"\\n--- Step 2c: Training DDM (Upgraded Model) ---\")\n    \n    ddm = BitstringDDM(model, num_timesteps=NUM_TIMESTEPS, device=device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    for epoch in range(NUM_EPOCHS):\n        total_loss = 0\n        for x_0, basis_id in dataloader:\n            optimizer.zero_grad()\n            loss = ddm.train_step(x_0, basis_id)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        if (epoch + 1) % 25 == 0: # Print every 25 epochs\n            avg_loss = total_loss / len(dataloader)\n            print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.6f}\")\n\n    print(\"--- Training Complete (Step 2 Done) ---\")\n\n    # --- Step 3: Sampling ---\n    generated_counts = []\n    \n    for basis_id in range(3):\n        num_samples = total_shots_per_basis[basis_id]\n        samples = ddm.sample(num_samples=num_samples, basis_id=basis_id)\n        \n        counts_counter = collections.Counter(samples)\n        counts = {'0': counts_counter.get(0, 0), '1': counts_counter.get(1, 0)}\n        \n        generated_counts.append(counts)\n\n    # --- Results ---\n    print(\"\\n--- Comparison: Original vs. DDM Denoised ---\")\n    for i in range(3):\n        label = BASIS_LABELS[i]\n        print(f\"\\nBasis {label}:\")\n        print(f\"  Original Noisy: {original_noisy_counts[i]}\")\n        print(f\"  DDM Denoised:   {generated_counts[i]}\")\n\n    print(\"\\nNext step (Step 4): Reconstruct a density matrix from the 'DDM Denoised' counts.\")\n\nif __name__ == \"__main__\":\n    run_ddm_poc()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:37:05.397596Z","iopub.execute_input":"2025-11-06T09:37:05.398548Z","iopub.status.idle":"2025-11-06T09:40:21.422015Z","shell.execute_reply.started":"2025-11-06T09:37:05.398511Z","shell.execute_reply":"2025-11-06T09:40:21.421112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for 4096 shots\n\n# --- Python Code ---\nimport numpy as np\nfrom qiskit.quantum_info import Statevector, state_fidelity, DensityMatrix\n\n# --- Pauli Matrices ---\nSIGMA_X = np.array([[0, 1], [1, 0]], dtype=complex)\nSIGMA_Y = np.array([[0, -1j], [1j, 0]], dtype=complex)\nSIGMA_Z = np.array([[1, 0], [0, -1]], dtype=complex)\nIDENTITY = np.array([[1, 0], [0, 1]], dtype=complex)\n\n# === START: TODO ===\n\n'''\nBasis X:\n  Original Noisy: {'0': 3785, '1': 311}\n  DDM Denoised:   {'0': 3857, '1': 239}\n\nBasis Y:\n  Original Noisy: {'0': 2152, '1': 1944}\n  DDM Denoised:   {'0': 2178, '1': 1918}\n\nBasis Z:\n  Original Noisy: {'0': 2067, '1': 2029}\n  DDM Denoised:   {'0': 2111, '1': 1985}\n\n\n'''\n\n# TODO 1:\n# Copy the 'DDM Denoised' counts from your Step 2/3 script output.\nddm_counts_X = {'0': 3857, '1': 239}   # <-- PASTE YOUR DDM 'X' COUNTS\nddm_counts_Y = {'0': 2178, '1': 1918}  # <-- PASTE YOUR DDM 'Y' COUNTS\nddm_counts_Z = {'0': 2111, '1': 1985}  # <-- PASTE YOUR DDM 'Z' COUNTS\n\n# TODO 2:\n# Copy the 'Noisy Fidelity' (the score to beat) from your Step 1 output.\nCLASSICAL_BASELINE_FIDELITY = 0.924072  # <-- PASTE YOUR BASELINE FIDELITY\n\n# === END: TODO ===\n\n\ndef calculate_expectation_value(counts):\n    \"\"\"\n    Calculates the expectation value <Z> from a Qiskit counts dictionary.\n    \n    <Z> = P(0) - P(1)\n    \"\"\"\n    total_shots = sum(counts.values())\n    if total_shots == 0:\n        return 0.0 # Avoid division by zero\n        \n    p_0 = counts.get('0', 0) / total_shots\n    p_1 = counts.get('1', 0) / total_shots\n    \n    return p_0 - p_1\n\ndef reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z):\n    \"\"\"\n    Reconstructs the 1-qubit density matrix rho using the formula:\n    rho = (1/2) * (I + <X>sigma_X + <Y>sigma_Y + <Z>sigma_Z)\n    \"\"\"\n    rho = 0.5 * (IDENTITY + \n                   exp_val_X * SIGMA_X + \n                   exp_val_Y * SIGMA_Y + \n                   exp_val_Z * SIGMA_Z)\n    \n    # Return as a Qiskit DensityMatrix object for fidelity calculation\n    return DensityMatrix(rho)\n\ndef run_reconstruction():\n    print(\"--- Step 4: Reconstructing DDM's Density Matrix ---\")\n\n    # 1. Define our target state again for fidelity calculation\n    target_state = Statevector.from_label('+')\n\n    # 2. Calculate expectation values from the DDM's counts\n    exp_val_X = calculate_expectation_value(ddm_counts_X)\n    exp_val_Y = calculate_expectation_value(ddm_counts_Y)\n    exp_val_Z = calculate_expectation_value(ddm_counts_Z)\n\n    print(f\"DDM Expectation Values:\")\n    print(f\"  <X> = {exp_val_X:.6f}\")\n    print(f\"  <Y> = {exp_val_Y:.6f}\")\n    print(f\"  <Z> = {exp_val_Z:.6f}\")\n\n    # 3. Reconstruct the density matrix\n    rho_ddm = reconstruct_rho(exp_val_X, exp_val_Y, exp_val_Z)\n    \n    print(f\"\\nDDM Reconstructed (rho_ddm):\\n{np.round(rho_ddm.data, 3)}\")\n\n    # 4. Calculate the final fidelity\n    ddm_fidelity = state_fidelity(target_state, rho_ddm)\n\n    print(\"\\n--- Step 5: Final Results & Comparison ---\")\n    print(f\"  Classical Baseline Fidelity: {CLASSICAL_BASELINE_FIDELITY:.6f}\")\n    print(f\"  DDM Denoised Fidelity:       {ddm_fidelity:.6f}\")\n\n    # 5. Print the verdict\n    if ddm_fidelity > CLASSICAL_BASELINE_FIDELITY:\n        improvement = ddm_fidelity - CLASSICAL_BASELINE_FIDELITY\n        print(f\"\\n*** SUCCESS! ***\")\n        print(f\"The DDM improved the fidelity by {improvement:.6f}.\")\n    else:\n        print(f\"\\n--- No Improvement ---\")\n        print(\"The DDM fidelity did not beat the classical baseline.\")\n        print(\"This may be due to a short training run or unlucky sampling.\")\n        print(\"Try re-running Step 2/3, or increasing NUM_EPOCHS.\")\n\nif __name__ == \"__main__\":\n    run_reconstruction()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:41:32.241789Z","iopub.execute_input":"2025-11-06T09:41:32.242724Z","iopub.status.idle":"2025-11-06T09:41:32.256400Z","shell.execute_reply.started":"2025-11-06T09:41:32.242690Z","shell.execute_reply":"2025-11-06T09:41:32.255527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The Big Picture: Our Goal\n\nOur goal is to see if a  Denoising Diffusion Model is better at \"cleaning\" noisy quantum data than the standard classical methods.\n\nIn other words, a contest: **\"Classical Tomography\" vs. \"DDM Tomography.\"**\n\n---\n\n## Step 1: The \"Experiment\" and \"Setting the Score\"\n\n\n\n\n1.  **Prepare the State:** We define a perfect 1-qubit target state, `|+>`. This is our \"ground truth.\"\n2.  **Run the \"Classical Race\":** We run Qiskit's built-in `StateTomography` on a *noisy* simulator (`FakeVigo`). This simulator adds errors, just like real hardware.\n3.  **Get the Classical Score:** This classical method analyzes the noisy data and produces `rho_noisy`, which has a fidelity of **`0.917969`**. This is the **baseline score to beat**.\n4.  **Get the Training Data:** The *same* noisy simulation gives us the `Counts` (the raw X, Y, Z measurement results). This noisy, messy data is the \"training data\" we will feed to our AI.\n\n\n\n---\n\n### Step 2: Training the AI (The \"Brain\")\n\nThis is where we build and train our \"brain\" (the model used for prediction).\n\n1.  **Load Data:** We \"unroll\" the `Noisy Counts`. For example, `{'X': {'0': 940, '1': 84}}` becomes a giant list containing 940 `0`s and 84 `1`s, where each bit is \"tagged\" with the `X-Basis` label.\n2.  **The \"Training Game\":** We play a game with our AI for 300 epochs:\n    * **Corrupt:** We take a *clean* bit from our dataset (e.g., `0`), and add a *random* amount of noise, flipping it (e.g., to `1`). This is the **Forward Diffusion** process.\n    * **Ask:** We show the model the noisy bit `1` and ask, \"I corrupted this bit at noise level `t=50` and it's from the `X-Basis`. What was the original?\"\n    * **Guess:** The model makes a prediction (e.g., `0`).\n    * **Learn:** We tell the AI, \"You were correct!\" (or \"You were wrong!\"), and it updates its neural network.\n\n\n\nAfter 300 epochs, our AI is incredibly good at this \"denoising\" game. It has learned the hidden rules, like \"If the basis is X, the clean bit should almost *always* be `0`.\"\n\n---\n\n### Step 3: Running the AI (The \"Generator\")\n\nNow we use our \"smart\" AI to generate *brand new, clean data*. We run the training game **in reverse**.\n\n1.  **Start with Garbage:** We create 1024 *completely random* bits (pure noise). This is `x_T` (time `T=100`).\n2.  **Denoise Step-by-Step:** We ask our AI, \"Here is a batch of garbage `x_100` for the `X-Basis`. What's one step cleaner (`x_99`)?\"\n3.  **Iterate:** The AI gives us `x_99`. We feed *that* back to the AI and ask for `x_98`. We repeat this 100 times, \"walking\" the data from pure noise back to a clean state. This is the **Reverse Diffusion** process.\n4.  **Get Clean Data:** Our final result, `x_0`, is a batch of 1024 *newly generated* bits that represent the AI's \"idea\" of a perfect, clean X-Basis measurement.\n5.  **Output:** We do this for all three bases (X, Y, Z) and get our `DDM Denoised` counts.\n\n\n\n---\n\n### Step 4: Building the \"AI's State\"\n\n\nWe takes the AI's *clean* counts and builds a quantum state from them.\n\n1.  **Get Clean Counts:** We paste in the `DDM Denoised` counts from Step 3.\n2.  **Calculate Expectation Values:** We convert the clean counts into three numbers:\n    * `DDM Denoised X {'0': 1020, '1': 4}` $\\rightarrow$ $\\langle X \\rangle = (1020 - 4) / 1024 = \\mathbf{0.992}$\n    * ...and so on for $\\langle Y \\rangle$ and $\\langle Z \\rangle$.\n3.  **Build the State:** We use the physics formula $\\rho = \\frac{1}{2}(I + \\langle X \\rangle\\sigma_X + \\langle Y \\rangle\\sigma_Y + \\langle Z \\rangle\\sigma_Z)$ to build the final `rho_ddm` (our AI's $2 \\times 2$ density matrix).\n\n---\n\n### Step 5: The \"Judgment Day\"\n\nThis is the end of the race. We compare the scores.\n\n1.  **Calculate DDM Fidelity:** We compare our AI's state (`rho_ddm`) to the perfect \"ground truth\" state (`|+>`) and get our final score: `DDM Denoised Fidelity`.\n2.  **Compare the Scores:** The script prints the final comparison:\n    * `Classical Baseline Fidelity: 0.917969`\n    * `DDM Denoised Fidelity:   0.909180` \n\n\n\nThat is the entire, end-to-end research pipeline, for now\n\n## Next steps\n1. Documentation\n        Need to document every thing used and decision made. This is provide option for introducing varioations. \n4. Metrics-> Runtime, fair comparison between both approaches, Finding ddm advantage\n        Advantage of DL based methods established in this paper. \n6. how to consider the basis for multile qubit\n7. Custom noise exploration\n       based on changing probabbilities of any particular basis (X,Y,Z) error occuring.\n5. classical method vs DL method\n        with LI method\n7. basis of measurement, shots considered\n       selective/ random basis considered to compare\n       what basis to try to use\n","metadata":{}}]}